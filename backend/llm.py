from openai import OpenAI
import json
from shared.config import OPENAI_API_KEY
from backend.prompts import conversion_to_sql_prompt, answer_formulation_prompt

client = OpenAI(api_key=OPENAI_API_KEY)


async def query_to_sql(query: str, schema: dict, language: str = "en") -> str:
    """
    This function converts a user query into a SQL query using the LLM.

    Args:
        query (str): the user query
        schema (dict): databse schema for LLM to understand the database structure
        language (str, optional): Defaults to "en".

    Returns:
        str: SQL query generated by the LLM
    """
    # Constructing prompt
    conversion_prompt = conversion_to_sql_prompt(query, schema, language)

    llm_response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {"role": "system", "content": "You are an expert SQL assistant."},
            {"role": "user", "content": conversion_prompt},
        ],
        temperature=0.0,
        response_format={"type": "json_object"},
    )

    # Extrecting SQL
    raw_response = llm_response.choices[0].message.content
    sql_json = json.loads(raw_response)
    sql_query = sql_json.get("sql")
    llm_error = sql_json.get("error", "")

    return sql_query, llm_error


async def answer_formulation(query: str, sql_result: dict, language: str = "en") -> str:
    """
    This fucntion formulates the answer based on the User query, SQL result and lang.
    Args:
        query (str): User query
        sql_result (dict): SQL result in dict format
        language (str, optional): Language to formulate the answer. Defaults to "en".
    Returns:
        str: Answer formulated by the LLM
    """
    # Constructing prompt
    prompt = answer_formulation_prompt(query, sql_result, language)

    llm_response = client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that summarizes SQL results.",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        response_format={"type": "json_object"},
    )

    return llm_response.choices[0].message.content
